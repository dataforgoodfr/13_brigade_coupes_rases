import os

import geopandas as gpd
import yaml
from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from utils.download import decompress_zip, download_file
from utils.logger import init_db_logger
from utils.s3 import S3Manager

# Configuration


load_dotenv()

## Logger
logger = init_db_logger("./logs/main.log")

## Chargement des param√®tres
with open("./config/config.yaml") as stream:
    configs = yaml.safe_load(stream)

## S3 Manager
s3_manager = S3Manager()

## Connexion √† la base de donn√©es
engine = create_engine(
    f"postgresql://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}@"
    f"{os.getenv('DB_HOST')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}"
)

# D√©finition des chemins de fichiers
download_path = configs["download_path"]
s3_prefix = configs["rivers"]["s3_prefix"]
s3_filename = configs["rivers"]["s3_filename"]
local_file_path = os.path.join(download_path, s3_filename)
geojson_zip_path = os.path.join(download_path, "CoursEau_geojson.zip")
geojson_path = os.path.join(download_path, "CoursEau_FXX.geojson")
geoparquet_path = os.path.join(download_path, "CoursEau.geoparquet")


def load_data_from_s3():
    """T√©l√©charge et charge les donn√©es depuis S3."""
    logger.info("‚úÖ T√©l√©chargement du fichier d√©j√† pr√©sent dans S3")
    s3_manager.download_from_s3(
        s3_key=os.path.join(s3_prefix, s3_filename),
        download_path=local_file_path,
    )
    return gpd.read_parquet(local_file_path).set_geometry("geom")


def download_and_process_data():
    """T√©l√©charge, transforme et enregistre les donn√©es."""
    logger.info("‚úÖ T√©l√©chargement du fichier depuis la source")
    download_file(configs["rivers"]["download_url"], filename=geojson_zip_path)

    logger.info("‚úÖ D√©compression du fichier")
    decompress_zip(input_file=geojson_zip_path, output_file=download_path)

    logger.info("‚úÖ Processing du fichier")
    data = (
        gpd.read_file(geojson_path)
        .rename(columns={"geometry": "geom"})
        .set_geometry("geom")
        .to_crs("epsg:4326")
    )

    data.to_parquet(geoparquet_path)

    return data


def write_to_database(data):
    """√âcrit les donn√©es dans la base PostgreSQL."""
    logger.info("‚úÖ √âcriture des donn√©es dans la base de donn√©es")
    data.to_postgis("rivers", engine, if_exists="replace", chunksize=5000)


if __name__ == "__main__":
    logger.info("üöÄ D√©marrage du script rivi√®re")

    try:
        # V√©rifie si le fichier est dans S3 et charge les donn√©es
        if s3_manager.file_check(s3_prefix, s3_filename):
            logger.info("‚úÖ Le fichier est dans S3")
            data = load_data_from_s3()
        else:
            logger.info("‚úÖ Le fichier n'est pas dans S3")
            data = download_and_process_data()

            logger.info("‚úÖ Export vers S3")

            """
            s3_manager.upload_to_s3(
                s3_key=os.path.join(s3_prefix, s3_filename),
                file_path=geoparquet_path,
            )
            """

        final_data = data[["CdOH", "TopoOH", "geom"]].rename(
            {"CdOH": "Id_river", "TopoOH": "Name_river"}
        )

        # √âcriture dans la base de donn√©es
        write_to_database(final_data)

        # Indexing spatial sur les g√©ometries
        with engine.connect() as conn:
            query = text("""
                CREATE INDEX rivers_geom_idx ON rivers USING GIST (geom);
            """)

            result = conn.execute(query)

        logger.info("‚úÖ Fin du pipeline")

    except Exception as e:
        logger.error(f"‚ùå Erreur dans le pipeline: {str(e)}", exc_info=True)
