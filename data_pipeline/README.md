# Data pipeline

## Data Pipeline using cron jobs
### File structure 

```bash
.
data_pipeline/
├── Dockerfile
├── config/
│   ├── config.yaml 
├── scripts/
│   ├── extract.py
│   ├── transform.py
│   ├── main.py
│   ├── utils/
│   │   ├── .env # temporary waiting the keypass
│   │   ├── PyKeePass.py # Should be updated by Cindy
│   │   ├── date_parser.py
│   │   ├── disjoin_set.py
│   │   ├── logging.py
│   │   ├── s3.py
# Mauricio Barra
# │   ├── tests/
# │   │   ├── __init__.py
# │   │   ├── data_contracts/
# │   │       ├── backend_data_contract.yml
# │   │   ├── data_quality_checks/
# │   │       ├── backend_data quality_check.py
├── entrypoint.sh
├── cron-job.sh
├── data_temp/ # generated by bash script DON'T CREATE THE FOLDER !
├── logs/ # generated by bash script DON'T CREATE THE FOLDER ! 
│   ├── brc_data_pipeline.log

```

### Pipeline installation
**1. Docker installation supposing that you're in data pipeline folder**

```bash
    docker build -t brc_data_pipeline . 
```

**2. Run the dockerfile**
```bash
    docker run -d --name cron_data_pipeline brc_data_pipeline 
```

## KeePass

- KeePass is the password manager used to store AWS credentials. These credentials are securely stored in the project's KeePass database.

- To use it, ensure that you have:

### The password for the KeePass file, stored in the `data_pipeline/.env` file locally  
Template of file to follow :  
````sh
KEEPASS_PASSWORD= votre_mot_de_passe # À compléter localement par chaque utilisateur
````

### The KeePass file, stored locally  

- Once these requirements are met, launch the `PyKeePass.py` file. The variables `access_key` and `secret_key` will contain the AWS S3 bucket access and secret keys.  


