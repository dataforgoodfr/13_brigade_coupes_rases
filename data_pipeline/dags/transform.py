import io
import os
import shutil
import rasterio
import logging
import numpy as np
import pandas as pd
from tqdm import tqdm
import geopandas as gpd
from pathlib import Path
from osgeo import gdal, ogr, osr


# Configuration des logs
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# Récupération du fichier S3
def load_from_S3(bucket_name, s3_key, s3_hook, download_path):
    if "temp_tif" not in os.listdir():
        os.makedirs("temp_tif", exist_ok=True)
    try:
        s3_hook.download_file(
            bucket_name=bucket_name, 
            key=s3_key, 
            local_path="dags/"+download_path, 
            preserve_file_name=True, 
            use_autogenerated_subdir=False
        )

        logger.info("✅ Fichier téléchargé depuis S3")
    except Exception as e:
        logger.error(f"❌ Erreur lors du téléchargement du fichier depuis S3: {e}")


# Regrouper par jours
def regroup_sufosat_days(
    input_tif_filepath: str | Path,
    output_tif_filepath: str | Path,
    start_day: pd.Timestamp,
    end_day: pd.Timestamp,
    **kwargs
) -> None:
    sufosat_start_day = pd.Timestamp(year=2014, month=4, day=3)
    start_days = (start_day - sufosat_start_day).days
    end_days = (end_day - sufosat_start_day).days

    # Open the input TIFF file
    with rasterio.open(input_tif_filepath) as src:
        # Copy metadata
        profile = src.profile
        profile.update(dtype=rasterio.uint8)

        # Open the output TIFF file
        with rasterio.open(output_tif_filepath, "w", **profile) as dst:
            # Read the data as a generator (window-by-window) to avoid out Of memory issues
            # since the total grid contains billions of points
            for _, window in tqdm(
                src.block_windows(),
                total=src.width
                * src.height
                // (src.block_shapes[0][0] * src.block_shapes[0][1]),
            ):
                # Read the block data for the first and only band
                data = src.read(1, window=window)

                # Apply the date filter to create binary image (0 or 1)
                binary_data = ((data >= start_days) & (data <= end_days)).astype(np.uint8)

                # Write processed block to new file
                dst.write(binary_data, 1, window=window)
                
    kwargs["ti"].xcom_push(key="regrouped_tif_path", value=output_tif_filepath)
    logger.info("✅ Fichier tif regroupé")


# Vectorisation du raster
def polygonize_tif(**kwargs): 
    raster_path = kwargs["ti"].xcom_pull(task_ids="transformation_pipeline.regroup_sufosat_days", key="regrouped_tif_path")
    vector_path = "dags/temp_shape/"
    filename = "mosaics_tropisco_warnings_france_date_2024.shp"   
    os.makedirs(os.path.dirname(vector_path+filename), exist_ok=True)

    # Ouvrir le raster
    raster_ds = gdal.Open(raster_path)
    if raster_ds is None:
        raise ValueError(f"Échec de l'ouverture du raster : {raster_path}")
    else:
        print("✅ Raster ouvert avec Gdal")

    srs = osr.SpatialReference()
    srs.ImportFromWkt(raster_ds.GetProjectionRef())

    driver = ogr.GetDriverByName("ESRI Shapefile")
    if driver is None:
        raise RuntimeError("Pilote Shapefile non disponible.")
    if os.path.exists(vector_path):
        driver.DeleteDataSource(vector_path)
    vector_ds = driver.CreateDataSource(vector_path)
    if vector_ds is None:
        raise RuntimeError(f"Échec de la création du Shapefile : {vector_path}")
    layer = vector_ds.CreateLayer("polygons", srs=srs, geom_type=ogr.wkbPolygon)
    if layer is None:
        raise RuntimeError("Échec de la création de la couche.")

    field_dn = ogr.FieldDefn("DN", ogr.OFTInteger)
    layer.CreateField(field_dn)

    band = raster_ds.GetRasterBand(1)
    mask_band = band

    # Appliquer la fonction Polygonize avec la bonne configuration
    result = gdal.Polygonize(band, mask_band, layer, 0, ["8CONNECTED=YES"])
    if result != 0:
        raise RuntimeError("Échec de la polygonisation.")

    # Nettoyer et fermer les fichiers
    layer = None
    vector_ds = None
    raster_ds = None

    kwargs["ti"].xcom_push(key="vector_path", value=vector_path)
    
    # Afficher les logs en print 
    logger.info("✅ Fichier shapefile créé")
    logger.info("✅ Couche SRS créée")
    logger.info("✅ Polygonisation du raster réussie")


# Imbrication de fonction
def convert_geometries_to_wkt(sufosat_2024):
    for col in sufosat_2024.columns:
            if col != "geometry" and sufosat_2024[col].dtype.name == "geometry":
                sufosat_2024[col] = sufosat_2024[col].apply(lambda geom: geom.wkt if geom else None)
    logger.info("✅ Conversion des géométries en WKT")

    geojson_str = sufosat_2024.to_json()
    logger.info("✅ Conversion str GEOJSON")
    
    return geojson_str


# Transformation du raster 
def process_geo_data(**kwargs):
    vector_path = kwargs["ti"].xcom_pull(task_ids="transformation_pipeline.polygonize_tif", key="vector_path")
    sufosat_2024: gpd.GeoDataFrame = gpd.read_file(vector_path)
    gdata = sufosat_2024
    
    # Opération géographiques sur le geodataframe sans jointure geo 
    gdata["area_ha"] = gdata.to_crs(epsg=3395).geometry.area / 10000
    logger.info("✅ Calcul de la surface en hectares")
    gdata["coordonnees"] = gdata.to_crs(epsg=3395).geometry.centroid
    logger.info("✅ Calcul des coordonnées")
    gdata_new = convert_geometries_to_wkt(gdata)

    shutil.rmtree("dags/temp_tif")
    shutil.rmtree("dags/temp_shape")

    kwargs["ti"].xcom_push(key="geojson", value=gdata_new)
