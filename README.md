# Brigade des Coupes Rases üå≥

- [Backend](./backend/README.md)
- [Frontend](./frontend/README.md)
- [Data Eng](./data_pipeline/README.md)
- [Analytics](./analytics/README.md)
- [Documentation](./doc/README.md)

# Contexte du Projet

La d√©forestation et les coupes rases ill√©gales repr√©sentent une menace majeure pour les √©cosyst√®mes et la biodiversit√©. Cependant, il existe un manque de transparence et de contr√¥le efficace sur ces pratiques, rendant difficile leur suivi et leur r√©gulation.

Canop√©e, une association engag√©e pour la protection des for√™ts, cherche √† automatiser la d√©tection des coupes rases abusives en utilisant un algorithme de surveillance satellite. Actuellement, les alertes g√©n√©r√©es doivent √™tre centralis√©es, analys√©es et valid√©es, mais ce processus reste manuel et fastidieux.

# Objectifs du Projet

L‚Äôobjectif est de d√©velopper une solution compl√®te pour :

- Automatiser le traitement des donn√©es des coupes rases d√©tect√©es par l‚Äôalgorithme existant (GlobEO).
- Cr√©er une base de donn√©es pour stocker et organiser les informations sur chaque coupe rase d√©tect√©e.
- Cr√©er une application permettant d‚Äôinteragir avec la base de donn√©es pour ajouter, modifier ou supprimer des informations sur chaque coupe rase d√©tect√©e.
- D√©velopper une interface de visualisation pour identifier les coupes rases ill√©gales et g√©n√©rer des statistiques exploitables.
  Optionellement :
- Repliquer l'identification de coupe rases (algorithme existant fourni par GlobEO) poour reduire le temps de mise a jour du processus existant.

# Contributing

## Pour commencer

1. [Rejoindre](https://dataforgood.fr/join) la communaut√© Data For Good
2. Sur le slack Data For Good, rejoindre le canal \_#13_brigade_coupes_rases et se pr√©senter
3. Remplir le [formulaire](https://noco.services.dataforgood.fr/dashboard/#/nc/form/da3564a9-5422-4810-a56f-26122c06dddc)
4. Explorer la documentation du projet. Familiarisez vous avec le projet, ses objectifs via [Outline](https://outline.services.dataforgood.fr/doc/presentation-du-projet-p8g6j1J3ZT). Notamment, vous trouverez les CR des premi√®res r√©unions avec Canop√©e qui sp√©cifient les avanc√©es du projet.

## Pour contribuer

Pour contribuer, vous pouvez demander un acc√®s au projet sur github.  Pour cela, contactez les responsables sur le slack Data For Good `#13_brigade_coupes_rases` .

Essayez de respecter les conventions de code et le style d'√©criture du projet:

  - feature/nom_de_la_feature pour une nouvelle fonctionnalit√©
  - chore/nom_du_chore pour une modification de code qui ne change pas l'interface utilisateur ou les fonctionnalit√©s existantes
  - hotfix/nom_du_hotfix pour une correction rapide


Chaque commit doit suivre la convention de style suivante :

  - Convention compl√®te de style, cheatsheet [HERE](https://gist.github.com/qoomon/5dfcdf8eec66a051ecd85625518cfd13)
  - Structure:
    - [Type] (optional scope): [Description]
    - [Optional Body]
    - [Optional Footer]
  - Exemple : chore(readme): ajouter d√©tails pour contribuer au repo

Cr√©ez une pull request.

- Pour faciliter la revue de la pull request :
  - Liez la pull request √† un ticket NocoDB en ajoutant le lien du ticket dans la description.
  - R√©digez une description d√©taill√©e de la pull request afin de fournir un maximum d‚Äôinformations sur les modifications apport√©es.



# Architecture du Projet (sujet √† am√©liorer et definir selon les expertises des volonteurs)

L'idee√®r du projet est de cr√©er une architecture modulaire qui permet d'automatiser le traitement des donn√©es, de stocker et organiser les informations dans une base de donn√©es, et de fournir une interface utilisateur pour interagir avec cette base de donn√©es. Voici un exemple possible de l'architecture :

1. **GlobEO Algorithme** : L‚Äôalgorithme GlobEO d√©tecte les coupes rases dans les images. T√©l√©charger les donn√©es de l‚Äôalgorithme GlobEO dans un format appropri√© (par exemple, CSV ou JSON).
2. **Traitement des Donn√©es** : (√Ä definir) Le traitement des donn√©es est effectu√© par un script Python pour manipuler et analyser les donn√©es. Selon les besoins, cela pourrait √©voluer (par exemple, orchestration, data quality, monitoring, etc.)
3. **Base de Donn√©es** : (√Ä definir) Base de donn√©es PostgreSQL avec PostGIS pour stocker les coupes rases et leurs m√©tadonn√©es spatiales. Cela aussie devrait faciliter le processement spatiale.
4. **Application Web** : (√Ä definir) Une application web Flask ou FastAPI est d√©velopp√©e pour g√©rer (op√©rations CRUD) les donn√©es relatives √† chaque coupe rase d√©tect√©e. (avec un service d'authentification pour les utilisateurs administrateurs).
5. **Interface de Visualisation** : (√Ä definir) Une interface web utilisant Leaflet ou Mapbox est cr√©√©e pour visualiser les coupes rases sur une carte. Framework aussi √† d√©finir.

## Structure du projet

```
üìÅ 13_brigade_coupes_rases
|
‚îú‚îÄ‚îÄ üìÅ backend/ (contient l'API et la gestion de la base de donn√©es)
|
‚îú‚îÄ‚îÄ üìÅ frontend/ (contient le code frontend pour la visualisation de donn√©es et les formulaires)
|
‚îú‚îÄ‚îÄ üìÅ data_pipeline/ (contient les scripts pour collecter et traiter les donn√©es)
|
‚îî‚îÄ‚îÄ üìÅ analytics/ (contient les scripts pour analyser et visualiser les donn√©es)
```


## Diagramme

Pour visualiser ce graph sur VS Code, vous pouvez installer des plugins comme "Markdown Preview Enhanced". 

Le graph est pas n√©cessairement complet, mais il donne une id√©e de l'architecture g√©n√©rale du projet. 

```mermaid
flowchart LR
    %% External Entities
    SOURCES[Public Data Sources<br>Monthly & One-Time] -->|Fetch Data| ETL
    Users[End Users] -->|Access<br>via Browser| FRONT

    %% Main Clever Cloud subgraph
    subgraph CC["CleverCloud"]
    
      %% Data Engineering Subgraph
      subgraph DE["Data Engineering"]
      ETL[Docker-based Ingestion Jobs]
      Cron[CleverCloud Cron Add-On]
      AutoScale[Autoscaling<br>CleverCloud]
      end

      %% Backend Subgraph
      subgraph BK["Backend"]
      BE[FastAPI Backend<br> - Docker]
      end

      %% Storage Subgraph
      subgraph ST["Storage"]
      S3[(S3 Object Storage)]
      DB[(PostgreSQL w/ PostGIS)]
      end

      %% Frontend Subgraph
      subgraph FE["Frontend"]
      FRONT[Frontend React/Vite<br>]
      end

      %% Observability Subgraph
      subgraph OBS["Observability"]
      Logs[CleverCloud Logs & Metrics]
      end

      %% Shared Environment Variables
      EnvVars[Built-in Env Variables]

    end

    %% Connections & Data Flows
    ETL -->|Raw/Intermediate Data| S3
    ETL -->|Transformed Data| DB
    Cron -->|Triggers| ETL
    AutoScale --> |Autoscales| ETL
    EnvVars -.-> ETL
    EnvVars -.-> BE
    EnvVars -.-> FRONT
    BE -->|Reads/Writes| DB
    BE --> |Reads/Writes Images| S3
    FRONT -->|API Calls| BE
    Logs -.-> ETL
    Logs -.-> BE
    Logs -.-> FRONT

    %% Styles for Readability
    style CC fill:#F0F9FF,stroke:#0369A1,stroke-width:2px,corner-radius:8px
    style DE fill:#ECFEFF,stroke:#05B4FE,stroke-width:1px,corner-radius:8px
    style BK fill:#FFF7ED,stroke:#F97316,stroke-width:1px,corner-radius:8px
    style FE fill:#FFFBEB,stroke:#F59E0B,stroke-width:1px,corner-radius:8px
    style OBS fill:#FAE8FF,stroke:#C026D3,stroke-width:1px,corner-radius:8px

    %% Custom Component Colors
    style DB fill:#CFFAFE,stroke:#0891B2,stroke-width:1px,color:#000000  %% Soft Teal for Storage
    style S3 fill:#CFFAFE,stroke:#0891B2,stroke-width:1px,color:#000000  %% Soft Teal for Storage

    style ETL fill:#FEE2E2,stroke:#DC2626,stroke-width:1px,color:#000000  %% Warm Coral for Docker
    style BE fill:#FEE2E2,stroke:#DC2626,stroke-width:1px,color:#000000  %% Warm Coral for Docker

    style Cron fill:#DBEAFE,stroke:#2563EB,stroke-width:1px,color:#000000  %% Cool Blue for CleverCloud
    style AutoScale fill:#DBEAFE,stroke:#2563EB,stroke-width:1px,color:#000000  %% Cool Blue for CleverCloud
    style Logs fill:#DBEAFE,stroke:#2563EB,stroke-width:1px,color:#000000  %% Cool Blue for CleverCloud

    style FRONT fill:#FEF3C7,stroke:#D97706,stroke-width:1px,color:#000000  %% Soft Amber for Frontend
```

### Utiliser Poetry

Poetry est un gestionnaire de paquets pour Python. Il permet d'installer, de mettre √† jour et de g√©rer les d√©pendances.
On utilise Poetry dans les repertoires backend, data_pipeline et analytics. Chaque repertoire a un fichier `pyproject.toml` qui contient les d√©pendances sp√©cifiques √† chaque subprojet.
Il y a aussi un fichier projet `pyproject.toml` √† la racine du projet qui contient les d√©pendances globales du projet.

Installer les d√©pendances:

  Il faut √™tre dans le r√©pertoire backend, data_pipeline ou analytics et ex√©cuter la commande suivante: 

    poetry install

Ajouter une d√©pendance (par r√©pertoire):

    poetry add pandas

Mettre √† jour les d√©pendances (par r√©pertoire):

    poetry update

### Installer Poetry (version 1.8.5)

Plusieurs [m√©thodes d'installation](https://python-poetry.org/docs/#installation) sont d√©crites dans la documentation de poetry dont:

- avec pipx
- avec l'installateur officiel

Chaque m√©thode a ses avantages et inconv√©nients. Par exemple, la m√©thode pipx n√©cessite d'installer pipx au pr√©able, l'installateur officiel utilise curl pour t√©l√©charger un script qui doit ensuite √™tre ex√©cut√© et comporte des instructions sp√©cifiques pour la completion des commandes poetry selon le shell utilis√© (bash, zsh, etc...).

L'avantage de pipx est que l'installation de pipx est document√©e pour linux, windows et macos. D'autre part, les outils install√©es avec pipx b√©n√©ficient d'un environment d'ex√©cution isol√©, ce qui est permet de fiabiliser leur fonctionnement. Finalement, l'installation de poetry, voire d'autres outils est relativement simple avec pipx.

Cependant, libre √† toi d'utiliser la m√©thode qui te convient le mieux ! Quelque soit la m√©thode choisie, il est important de ne pas installer poetry dans l'environnement virtuel qui sera cr√©√© un peu plus tard dans ce README pour les d√©pendances de la base de code de ce repo git.

### Installation de Poetry avec pipx

Suivre les instructions pour [installer pipx](https://pipx.pypa.io/stable/#install-pipx) selon ta plateforme (linux, windows, etc...)

Par exemple pour Ubuntu 23.04+:

    sudo apt update
    sudo apt install pipx
    pipx ensurepath

[Installer Poetry avec pipx](https://python-poetry.org/docs/#installing-with-pipx):

    pipx install poetry

### Installation de Poetry avec l'installateur officiel

L'installation avec l'installateur officiel n√©cessitant quelques √©tapes suppl√©mentaires,
se r√©f√©rer √† la [documentation officielle](https://python-poetry.org/docs/#installing-with-the-official-installer).

### Utiliser un venv python

    python3 -m venv .venv

    source .venv/bin/activate

### Lancer les precommit-hook localement

[Installer les precommit](https://pre-commit.com/)

    poetry run pre-commit run --all-files

## Gestion des secrets

Les secrets partag√©s entre les membres sont stock√©s dans une [base de donn√©es keepass](./keepass/secrets.kdbx).  
Pour installer keepass suivez ce [lien](https://keepass.info/index.html).  
Un mot de passe est n√©cessaire pour ouvrir la base de donn√©es, lire ou modifier les secrets. Pour r√©cuperer le mot de passe contactez directement les responsables de sous-√©quipes.

### Bonnes pratiques

Consid√©rez la base de donn√©es keepass comme √©tant la golden source de tous les secrets du projet.  
Chaque secret utilis√©s dans le projet doit √™tre r√©f√©renc√©s dans le keepass.  
Exemples de secrets √† utiliser dans la base : mot de passe du compte g√©rant l'infrastructure cloud, CI/CD, cl√©s d'API, chaines de connection pour base de donn√©es etc ...